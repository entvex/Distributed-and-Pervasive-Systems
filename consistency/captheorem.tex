\section{The CAP theorem}

In-depth about CAP. Maybe draw our own picture of the CAP triangle. 

\section{PACELC}
The PACELC theorem is a addition to the CAP theorem. In CAP, one must choose between high availability and consistency. This leaves no option for mission-critical but to sacrifice consistency because high availability is a must. This logic is not right because the P in CAP is a mix of \textit{partition tolerance} and a \textit{actual network partition}. Therefore it is wrong to assume the systems that reduce consistency in the absence of any partitions are doing it because of CAP. PACELC handles the cases when a network partition have or not happened.

\noindent The theorem states: If there is a partition \textbf{P}, how does the system trade off availability \textbf{A} and consistency \textbf{C}. Else, when the system is running normally in
the absence of partitions, how does the system trade off latency \textbf{L}
and consistency \textbf{C}? \textbf{( REF TIL Fischer DIPS S08 Consistency Slides.pdf side 13 ).}

\noindent Apache's \textbf{Cassandra} database system is a PA/EL system. If a partition happens, it will give up consistency for availability, and during normal operation gives up consistency for lower latency. Google's \textbf{BigTable}
follows ACID\footnote{ACID (Atomicity, Consistency, Isolation, Durability) is a set of properties of database transactions intended to guarantee validity even in the event of errors, power failures, etc.} hence PC/EC. It will refuse to give up consistency and pay the price in terms availability and latency costs to achieve it. \textbf{MongoDB} can be classified as a PA/EC system. In the default configuration, the system guarantees reads and writes to be consistent. When it faces a  \textbf{( REF TIL 2012 Abadi ConsistencyTradeoffsInModernDistributedDatabaseSystemDesign.pdf side 42 ).}
%TODO Denne fucking table vil ikke st√• under texten!?!?!
When a system have a network partition happen to it there is a few options for replication the data.
\begin{enumerate}[H]
	\item Data updates sent to all replicas at the same time.
	\begin{enumerate}
		\item Using a preprocessing layer gives us consistency but increased latency.
		\item Not using a preprocessing layer will decrease latency but can only offer eventual consistency.
	\end{enumerate}
	\item Data updates sent to an agreed-upon location first
	\begin{enumerate}
		\item Synchronous the master node waits until updates made it to the replicas. Therefor gaining consistency but pays latency.
		\item Asynchronous treats the update as if it were completed before being sent to a replica.
	\end{enumerate}
	\item Data updates sent to an arbitrary location first.
		\begin{enumerate}
		\item Synchronous then the latency problems of (2)(a) are present.
		\item Asynchronous consistency problems same as (1) and (2)(b) are present.
	\end{enumerate}
\end{enumerate}

\begin{table}[H]
	\centering
\begin{tabular}{|c|c|c|c|c|}
	\hline 
	System & A & C & L & C \\ 
	\hline 
	Cassandra & X &  & X &  \\ 
	\hline 
	BigTable &  & X &  & X \\ 
	\hline 
	MongoDB &  & X & X &  \\ 
	\hline 
\end{tabular}
  \caption{This is my one big table} \label{tab:PACELC}
\end{table}